{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b242678",
   "metadata": {},
   "source": [
    "# MSDS 534: Statistical Learning - Homework 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f7a3ce",
   "metadata": {},
   "source": [
    "In this problem, we analyze the `Bikeshare` dataset (details [here](https://archive.ics.uci.edu/dataset/275/bike+sharing+dataset)).\n",
    "\n",
    "The `Bikeshare` data contains the hourly and daily count of rental bikes between years 2011 and 2012 in the Capital bikeshare system in Washington D.C., along with weather and seasonal information.\n",
    "\n",
    "Our response data $\\{y_i\\}_{i=1}^n$ are the total number of bikers at each time period and our input data $\\{\\mathbf{x}_i=(x_{i1},\\dots, x_{ip})\\}_{i=1}^n$ consists of the features:\n",
    "- hour of the day\n",
    "- day of the year\n",
    "- month of the year\n",
    "- season (Winter, Spring, Summer, Fall)\n",
    "- weekday (Sun, Mon, Tues, Wed, Thurs, Fri, Sat)\n",
    "- holiday? (Yes=1, No=0)\n",
    "- workday? (Yes=1, No=0)\n",
    "- weather\n",
    "- temperature (normalized, in Celcius)\n",
    "- atemp (normalized feeling temperature, in Celcius)\n",
    "- humidity\n",
    "- windspeed\n",
    "\n",
    "Before beginning this problem, download `bikeshare.csv` from Canvas (in `Files/homeworks/hw1`). Save `bikeshare.csv` in the same directory (folder) as this Jupyter notebook.\n",
    "\n",
    "The following cell loads the packages you need for this problem and loads the `Bikeshare` data.\n",
    "\n",
    "The cell also splits the data into train and test data, stored as `numpy` objects:\n",
    "- `x_train_np` \n",
    "- `y_train_np`\n",
    "- `x_test_np` \n",
    "- `y_test_np`\n",
    "\n",
    "In this problem, we will compare different models for predicting $Y_i$ from $\\mathbf{x}_i$. Complete the questions and fill in the code where it states `[INSERT CODE]`. To grade your homeworks, we will run your completed Jupyter notebook and grade the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a5b032",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import random\n",
    "random.seed(10)\n",
    "torch.manual_seed(10)\n",
    "\n",
    "bike_data = pd.read_csv('bikeshare.csv')\n",
    "y = bike_data['bikers']\n",
    "x = bike_data.iloc[:, 1:]\n",
    "\n",
    "n = bike_data.shape[0]\n",
    "\n",
    "# get training and test indicies\n",
    "inds = np.random.binomial(1, 0.8, size=n)\n",
    "n_train = sum(inds)\n",
    "n_test = len(inds)-sum(inds)\n",
    "\n",
    "# turn pandas df to numpy\n",
    "x_np = x.to_numpy()\n",
    "y_np = y.to_numpy()\n",
    "\n",
    "x_np = x_np.astype(np.float32)\n",
    "y_np = y_np.astype(np.float32)\n",
    "\n",
    "# separate into train/test data\n",
    "x_train_np = x_np[inds==1, :]\n",
    "y_train_np = y_np[inds==1]\n",
    "x_test_np = x_np[inds==0, :]\n",
    "y_test_np = y_np[inds==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c7be5c",
   "metadata": {},
   "source": [
    "### Part 1: Poisson regression\n",
    "\n",
    "We first use a generalized linear model to fit our data. As our response is count data (i.e. number of bikes), we use a Poisson regression model:\n",
    "$$Y_i \\sim \\text{Poisson}(\\exp(\\mathbf{x}_i^T\\boldsymbol{\\beta})), \\quad \\boldsymbol{\\beta}\\in\\mathbb{R}^p, i = 1,\\dots, n.$$\n",
    "\n",
    "_Q1: Fit this Poisson regression model to `x_train_np` and `y_train_np` using `sklearn.linear_model.PoissonRegressor()`. Using the fitted model, get predictions from `x_test_np`. Print the mean squared error between predicted responses and `y_test_np`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2b87db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do poisson regression\n",
    "[INSERT CODE]\n",
    "\n",
    "mse = [INSERT CODE]\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48daa7cc",
   "metadata": {},
   "source": [
    "_Q2: For the test data, plot predicted responses (y-axis) vs actual responses (x-axis). Your plot look similar to:_\n",
    "\n",
    "![this](poisson_linear.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859fc284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot y vs y_hat\n",
    "mse_display = np.format_float_scientific(mse, precision=2,trim='k')\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "fig.set_size_inches(6, 4)\n",
    "ax.plot[INSERT CODE], [INSERT CODE], '.', label=\"observed\")\n",
    "ax.set_title('Poisson Regression (Linear). MSE = ' + str(mse_display))\n",
    "ax.set_xlabel('Actual Y')\n",
    "ax.set_ylabel('Predicted Y')\n",
    "plt.savefig('poisson_linear.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd07947b",
   "metadata": {},
   "source": [
    "## Part 2: Poisson regression using neural networks\n",
    "\n",
    "Let's now try Poisson regression, but with a neural network link function. The model is:\n",
    "\n",
    "$$Y_i \\sim \\text{Poisson}(\\exp(f(\\mathbf{x}_i, \\boldsymbol{\\theta})))$$\n",
    "where $f$ is a neural network and $\\boldsymbol{\\theta}$ are the weights and biases of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbab1edb",
   "metadata": {},
   "source": [
    "_Q1: Convert `x_train_np, y_train_np, x_test_np, y_test_np` to `torch` tensors._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c582ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert numpy to torch tensors\n",
    "x_train = [INSERT CODE]\n",
    "y_train = [INSERT CODE]\n",
    "x_test = [INSERT CODE]\n",
    "y_test = [INSERT CODE]\n",
    "\n",
    "y_train = y_train[:, None] # make n_train x 1 matrix (this is helpful for future calculations)\n",
    "y_test = y_test[:, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80dfa0b",
   "metadata": {},
   "source": [
    "_Q2: Create a `TensorDataset` called `train_data` from the training data. Create a `DataLoader` called `train_loader` for this `train_data`. Set `batch_size=10` and `shuffle=True`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef3bd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [INSERT CODE]\n",
    "train_loader = [INSERT CODE]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d31602",
   "metadata": {},
   "source": [
    "_Q3: Create a neural network class called `PoissonNet` by completing in the following code. Note: the probability mass function for $Y_i$ is:_\n",
    "$$P(Y_i=y_i|f_{\\theta}(\\mathbf{x}_i)) = \\frac{\\exp(y_if_{\\theta}(\\mathbf{x}_i))\\exp(-e^{f_{\\theta}(\\mathbf{x}_i)})}{y_i!}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81633eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the neural network\n",
    "class PoissonNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(PoissonNet, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # create a neural network with below hidden node dimensions and ReLU activations\n",
    "        # hidden_dim = [100, 100, 50]\n",
    "\n",
    "        [INSERT CODE]\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        [INSERT CODE]\n",
    "\n",
    "        return out\n",
    "\n",
    "    def loss_fn(self, x, y):\n",
    "\n",
    "        # write a loss function for the Poisson model above\n",
    "        [INSERT CODE]\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0315c4",
   "metadata": {},
   "source": [
    "_Q4: Fit `PoissonNet` to the training data by completing the following code._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1864b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model\n",
    "input_dim = [INSERT CODE]\n",
    "output_dim = [INSERT CODE]\n",
    "model = PoissonNet(input_dim=input_dim, output_dim=output_dim)\n",
    "\n",
    "lr = 0.001\n",
    "epochs = 100\n",
    "\n",
    "# create optimizer with Adam\n",
    "optimizer = [INSERT CODE]\n",
    "\n",
    "# set model to training mode\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "\n",
    "    # iterate over mini-batches of the data\n",
    "    for [INSERT CODE] in [INSERT CODE]:\n",
    "\n",
    "        # compute the loss for current mini-batch\n",
    "        loss = [INSERT CODE]\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print('epoch: ', epoch, 'loss:', f\"{epoch_loss:.3}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca9a135",
   "metadata": {},
   "source": [
    "_Q5: For the test data, plot predicted responses (y-axis) vs actual responses (x-axis). Your plot look similar to the below image. What do you notice compared to the results from Part 1?_\n",
    "\n",
    "![this](poisson_neural.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e27f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions from model \n",
    "y_pred = [INSERT CODE]\n",
    "\n",
    "# convert y_pred back to numpy and flatten\n",
    "y_pred_np = torch.flatten(y_pred).detach().numpy()\n",
    "\n",
    "mse = [INSERT CODE]\n",
    "\n",
    "mse_display = np.format_float_scientific(mse, precision=2,trim='k')\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "fig.set_size_inches(6, 4)\n",
    "ax.plot([INSERT CODE], [INSERT CODE], '.', label=\"observed\")\n",
    "ax.set_title('Poisson Regression (Neural Network). MSE = ' + str(mse_display))\n",
    "ax.set_xlabel('Actual Y')\n",
    "ax.set_ylabel('Predicted Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da5f60c",
   "metadata": {},
   "source": [
    "## Part 3: Negative binomial regression using neural networks\n",
    "\n",
    "For the Poisson distribution, the mean = the variance. This may not be a good assumption. A more flexible count model for $Y_i$ is the negative binomial distribution. In this section, we fit a negative binomial neural network model to the data.\n",
    "\n",
    "_Q1: Create a neural network class called `NBNet` by completing the following code._\n",
    "\n",
    "Note that the negative binomial probability mass function is:\n",
    "$$P(Y_i=y_i) = \\frac{\\Gamma(y_i + r(\\mathbf{x}_i,\\boldsymbol{\\theta}))}{y_i!\\Gamma(r(\\mathbf{x_i},\\boldsymbol{\\theta}))}[1-p(\\mathbf{x}_i,\\boldsymbol{\\theta})]^{y_i} [p(\\mathbf{x}_i,\\boldsymbol{\\theta})]^{r(\\mathbf{x_i},\\boldsymbol{\\theta})}.$$\n",
    "\n",
    "We also have $$\\mathbb{E}[Y_i|\\mathbf{x}_i] = \\frac{r(\\mathbf{x}_i,\\boldsymbol{\\theta})[1- p(\\mathbf{x}_i,\\boldsymbol{\\theta})]}{p(\\mathbf{x}_i,\\boldsymbol{\\theta})}.$$\n",
    "\n",
    "Helpful `torch` functions:\n",
    "- `torch.lgamma`: computes log of $\\Gamma$ function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e05c976",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NBNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(NBNet, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        hidden_dim = [100, 100]\n",
    "        r_dim = [50]\n",
    "        p_dim = [50]\n",
    "\n",
    "        # create backbone\n",
    "        current_dim = input_dim\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in [INSERT CODE]:\n",
    "            hdim = [INSERT CODE]\n",
    "            self.layers.append(nn.Linear([INSERT CODE], [INSERT CODE]))\n",
    "            current_dim = [INSERT CODE]\n",
    "\n",
    "        # create r head\n",
    "        core_dim = hidden_dim[-1]\n",
    "        current_dim = core_dim\n",
    "        self.r_layers = nn.ModuleList()\n",
    "        for i in range(len(r_dim)):\n",
    "            hdim = [INSERT CODE]\n",
    "            self.r_layers.append(nn.Linear(current_dim, hdim))\n",
    "            current_dim = [INSERT CODE]\n",
    "        self.r_layers.append(nn.Linear(current_dim, [INSERT CODE]))\n",
    "\n",
    "        # create p head\n",
    "        current_dim = core_dim\n",
    "        self.p_layers = nn.ModuleList()\n",
    "        for i in [INSERT CODE]:\n",
    "            hdim = [INSERT CODE]\n",
    "            self.p_layers.append(nn.Linear(current_dim, hdim))\n",
    "            current_dim = [INSERT CODE]\n",
    "        self.p_layers.append(nn.Linear(current_dim, [INSERT CODE]))\n",
    "\n",
    "    def core_net(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = F.relu(layer(x))\n",
    "        return x\n",
    "\n",
    "    def r_net(self, x):\n",
    "\n",
    "        [INSERT CODE]\n",
    "\n",
    "        out = torch.exp([INSERT CODE]) ## r needs to be > 0\n",
    "        return out\n",
    "\n",
    "    def p_net(self, x):\n",
    "\n",
    "        [INSERT CODE]\n",
    "\n",
    "        out = F.sigmoid([INSERT CODE]) ## p needs to be [0,1]\n",
    "        return out\n",
    "\n",
    "    def forward(self, x):\n",
    "        r = [INSERT CODE]\n",
    "        p = [INSERT CODE]\n",
    "\n",
    "        return r, p\n",
    "\n",
    "    def loss_fn(self, x, y):\n",
    "        r, p = self.forward(x)\n",
    "        \n",
    "        [INSERT CODE]\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8864b5",
   "metadata": {},
   "source": [
    "_Q2: Fit `NBNet` to the data by completing the following code._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecce11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate NBNet\n",
    "\n",
    "input_dim = [INSERT CODE]\n",
    "output_dim = [INSERT CODE]\n",
    "\n",
    "model = NBNet(input_dim=input_dim, output_dim=output_dim)\n",
    "\n",
    "lr = 0.001\n",
    "epochs = 100\n",
    "\n",
    "# create optimizer with Adam\n",
    "optimizer = [INSERT CODE]\n",
    "\n",
    "# set model to training mode\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "\n",
    "    # iterate over mini-batches of the data\n",
    "    for [INSERT CODE] in [INSERT CODE]:\n",
    "\n",
    "        # compute the loss for current mini-batch\n",
    "        loss = [INSERT CODE]\n",
    "        \n",
    "        # take gradient\n",
    "        [INSERT CODE]\n",
    "        # update parameters\n",
    "        [INSERT CODE]\n",
    "        # reset gradients to zero\n",
    "        [INSERT CODE]\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print('epoch: ', epoch, 'loss:', f\"{epoch_loss:.3}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceb079b",
   "metadata": {},
   "source": [
    "_Q3: Plot predicted responses (y-axis) vs actual responses (x-axis). Your plot look similar to the below image. What do you notice compared to the results from Parts 1 and 2?_\n",
    "\n",
    "![this](neg_binom.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a35c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions from model \n",
    "y_pred = [INSERT CODE]\n",
    "\n",
    "# convert y_pred back to numpy and flatten\n",
    "y_pred_np = torch.flatten(y_pred).detach().numpy()\n",
    "\n",
    "mse = [INSERT CODE]\n",
    "\n",
    "mse_display = np.format_float_scientific(mse, precision=2,trim='k')\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "fig.set_size_inches(6, 4)\n",
    "ax.plot([INSERT CODE], [INSERT CODE], '.', label=\"observed\")\n",
    "ax.set_title('Negative Binomial Regression (Neural Network). MSE = ' + str(mse_display))\n",
    "ax.set_xlabel('Actual Y')\n",
    "ax.set_ylabel('Predicted Y')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
